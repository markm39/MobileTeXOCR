{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7T11Z0kvX1Lq"
      },
      "source": [
        "# Handwritten LaTeX OCR Training\n",
        "\n",
        "Train the unified text spotting model on Google Colab with H100/A100."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vGL7EBAuX1Lr",
        "outputId": "31853e69-e5b7-426d-e844-91a0c6d6a1ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Jan 22 14:50:56 2026       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  NVIDIA A100-SXM4-80GB          Off |   00000000:00:05.0 Off |                    0 |\n",
            "| N/A   34C    P0             56W /  400W |       0MiB /  81920MiB |      0%      Default |\n",
            "|                                         |                        |             Disabled |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LM4H2OfrX1Lr",
        "outputId": "353afea9-bb1f-4a71-b82e-72c0325d69af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9dAdecmeX1Lr",
        "outputId": "28e4c051-51e1-4120-c7c8-19fe1a9bc28a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'MobileTeXOCR' already exists and is not an empty directory.\n",
            "/content/MobileTeXOCR\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/markm39/MobileTeXOCR.git\n",
        "%cd MobileTeXOCR"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd /content/MobileTeXOCR && git pull\n"
      ],
      "metadata": {
        "id": "RcT6PFe_TQvs",
        "outputId": "88da3ecc-685f-4d3f-a537-6ce84b319f81",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "remote: Enumerating objects: 13, done.\u001b[K\n",
            "remote: Counting objects:   7% (1/13)\u001b[K\rremote: Counting objects:  15% (2/13)\u001b[K\rremote: Counting objects:  23% (3/13)\u001b[K\rremote: Counting objects:  30% (4/13)\u001b[K\rremote: Counting objects:  38% (5/13)\u001b[K\rremote: Counting objects:  46% (6/13)\u001b[K\rremote: Counting objects:  53% (7/13)\u001b[K\rremote: Counting objects:  61% (8/13)\u001b[K\rremote: Counting objects:  69% (9/13)\u001b[K\rremote: Counting objects:  76% (10/13)\u001b[K\rremote: Counting objects:  84% (11/13)\u001b[K\rremote: Counting objects:  92% (12/13)\u001b[K\rremote: Counting objects: 100% (13/13)\u001b[K\rremote: Counting objects: 100% (13/13), done.\u001b[K\n",
            "remote: Compressing objects:  50% (1/2)\u001b[K\rremote: Compressing objects: 100% (2/2)\u001b[K\rremote: Compressing objects: 100% (2/2), done.\u001b[K\n",
            "remote: Total 8 (delta 5), reused 8 (delta 5), pack-reused 0 (from 0)\u001b[K\n",
            "Unpacking objects:  12% (1/8)\rUnpacking objects:  25% (2/8)\rUnpacking objects:  37% (3/8)\rUnpacking objects:  50% (4/8)\rUnpacking objects:  62% (5/8)\rUnpacking objects:  75% (6/8)\rUnpacking objects:  87% (7/8)\rUnpacking objects: 100% (8/8)\rUnpacking objects: 100% (8/8), 4.34 KiB | 1.45 MiB/s, done.\n",
            "From https://github.com/markm39/MobileTeXOCR\n",
            "   69dce9d7..630eafc6  main       -> origin/main\n",
            "Updating 69dce9d7..630eafc6\n",
            "Fast-forward\n",
            " models/decoder/unified_decoder.py | 280 \u001b[32m+++++++++++++++++++++++++++\u001b[m\u001b[31m-----------\u001b[m\n",
            " models/full_model.py              |   4 \u001b[32m+\u001b[m\u001b[31m-\u001b[m\n",
            " tests/test_kv_cache.py            | 116 \u001b[32m++++++++++++++++\u001b[m\n",
            " 3 files changed, 320 insertions(+), 80 deletions(-)\n",
            " create mode 100644 tests/test_kv_cache.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "xgKt3q4kX1Lr"
      },
      "outputs": [],
      "source": [
        "!pip install -q torch torchvision pillow numpy pyyaml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "B-vk-0NKX1Lr"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.insert(0, '/content/MobileTeXOCR')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19E0f5nqX1Lr"
      },
      "source": [
        "## Dataset Setup\n",
        "\n",
        "Choose ONE option below:\n",
        "- **Option A**: Create dummy data (for testing pipeline)\n",
        "- **Option B**: Download real datasets (for actual training)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2VPGVMVX1Lr"
      },
      "source": [
        "### Option A: Create Dummy Data (for testing)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0H6yJV3iX1Lr",
        "outputId": "3bd83a17-9801-434c-ac25-faa0a3518c32"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created 100 train samples in ./data/hme100k/train/images\n",
            "Created 20 val samples in ./data/hme100k/val/images\n",
            "Dummy dataset created!\n"
          ]
        }
      ],
      "source": [
        "# Create dummy dataset for testing the pipeline\n",
        "import os\n",
        "import json\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "\n",
        "def create_dummy_dataset(base_dir, num_train=100, num_val=20):\n",
        "    \"\"\"Create dummy handwritten math images for testing.\"\"\"\n",
        "\n",
        "    expressions = [\n",
        "        'x^2', 'y^2', 'x+y', 'a-b', '\\\\frac{1}{2}', '\\\\sqrt{x}',\n",
        "        'x^2+y^2', 'a^2-b^2', '\\\\alpha', '\\\\beta', '\\\\gamma',\n",
        "        '\\\\sum_{i=1}^{n}', '\\\\int_0^1', 'e^x', '\\\\pi r^2',\n",
        "        '\\\\frac{a}{b}', 'x_1', 'y_2', 'z^n', '\\\\theta'\n",
        "    ]\n",
        "\n",
        "    for split, num_samples in [('train', num_train), ('val', num_val)]:\n",
        "        img_dir = os.path.join(base_dir, 'hme100k', split, 'images')\n",
        "        os.makedirs(img_dir, exist_ok=True)\n",
        "\n",
        "        labels = {}\n",
        "        for i in range(num_samples):\n",
        "            # Create white image\n",
        "            img = Image.new('RGB', (384, 384), 'white')\n",
        "            draw = ImageDraw.Draw(img)\n",
        "\n",
        "            # Draw expression (simplified rendering)\n",
        "            expr = expressions[i % len(expressions)]\n",
        "            # Draw some random strokes to simulate handwriting\n",
        "            import random\n",
        "            random.seed(i)\n",
        "            x_start = random.randint(50, 150)\n",
        "            y_start = random.randint(150, 200)\n",
        "\n",
        "            # Simple text (in real data this would be actual handwriting)\n",
        "            try:\n",
        "                font = ImageFont.truetype('/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf', 40)\n",
        "            except:\n",
        "                font = ImageFont.load_default()\n",
        "\n",
        "            # Draw display version\n",
        "            display_text = expr.replace('\\\\', '').replace('{', '').replace('}', '').replace('_', '').replace('^', '')\n",
        "            draw.text((x_start, y_start), display_text, fill='black', font=font)\n",
        "\n",
        "            # Save image\n",
        "            img_name = f'sample_{i:04d}.png'\n",
        "            img.save(os.path.join(img_dir, img_name))\n",
        "            labels[img_name] = expr\n",
        "\n",
        "        # Save labels\n",
        "        labels_file = os.path.join(base_dir, 'hme100k', split, 'labels.json')\n",
        "        with open(labels_file, 'w') as f:\n",
        "            json.dump(labels, f, indent=2)\n",
        "\n",
        "        print(f'Created {num_samples} {split} samples in {img_dir}')\n",
        "\n",
        "# Create dummy data\n",
        "create_dummy_dataset('./data', num_train=100, num_val=20)\n",
        "print('Dummy dataset created!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VlVnJPuNX1Ls"
      },
      "source": [
        "### Option B: Download Real Datasets (for actual training)\n",
        "\n",
        "Run ONE or more of the cells below to download real data. MathWriting is recommended as the primary dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U-NWZzw2X1Ls",
        "outputId": "d1b49814-cf94-46eb-c014-4461ace0fd3d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mathwriting.tgz     100%[===================>]   2.88G  24.0MB/s    in 2m 10s  \n",
            "MathWriting directory structure:\n",
            "total 620228\n",
            "drwxr-xr-x 7 root   root       4096 Jan 22 14:09 .\n",
            "drwxr-xr-x 5 root   root       4096 Jan 22 14:53 ..\n",
            "-rw-r----- 1 218859 89939      7780 Jan 31  2024 readme.md\n",
            "drwxr-x--- 2 218859 89939    299008 Jan 31  2024 symbols\n",
            "-rw-r----- 1 218859 89939    523063 Jan 31  2024 symbols.jsonl\n",
            "drwxr-x--- 2 218859 89939  18567168 Jan 31  2024 synthetic\n",
            "-rw-r----- 1 218859 89939 604019016 Jan 31  2024 synthetic-bboxes.jsonl\n",
            "drwxr-x--- 2 218859 89939    380928 Jan 31  2024 test\n",
            "drwxr-x--- 2 218859 89939  10543104 Jan 31  2024 train\n",
            "drwxr-x--- 2 218859 89939    741376 Jan 31  2024 valid\n"
          ]
        }
      ],
      "source": [
        "# Download MathWriting dataset (230K human + 400K synthetic samples, 2.9GB)\n",
        "# This is the largest handwritten math expression dataset\n",
        "!mkdir -p data/mathwriting\n",
        "!wget -q --show-progress https://storage.googleapis.com/mathwriting_data/mathwriting-2024.tgz -O mathwriting.tgz\n",
        "!tar -xzf mathwriting.tgz -C data/\n",
        "!rm mathwriting.tgz\n",
        "\n",
        "# Check structure and reorganize if needed\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# The tarball extracts to mathwriting-2024/, we need mathwriting/\n",
        "if os.path.exists('data/mathwriting-2024') and not os.path.exists('data/mathwriting/train'):\n",
        "    # Move contents\n",
        "    for item in os.listdir('data/mathwriting-2024'):\n",
        "        src = f'data/mathwriting-2024/{item}'\n",
        "        dst = f'data/mathwriting/{item}'\n",
        "        if os.path.exists(dst):\n",
        "            shutil.rmtree(dst) if os.path.isdir(dst) else os.remove(dst)\n",
        "        shutil.move(src, dst)\n",
        "    os.rmdir('data/mathwriting-2024')\n",
        "\n",
        "print('MathWriting directory structure:')\n",
        "!ls -la data/mathwriting/ | head -20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "R0iEkajfX1Ls"
      },
      "outputs": [],
      "source": [
        "# # Download CROHME from Kaggle (requires Kaggle API key)\n",
        "# !pip install kaggle\n",
        "# !mkdir -p ~/.kaggle\n",
        "# # Upload your kaggle.json or set credentials\n",
        "# !kaggle datasets download -d xainano/handwrittenmathsymbols\n",
        "# !unzip -q handwrittenmathsymbols.zip -d data/crohme/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "FcF3LBt2X1Ls"
      },
      "outputs": [],
      "source": [
        "# # Alternative: Download from HuggingFace (if available)\n",
        "# !pip install huggingface_hub\n",
        "# from huggingface_hub import snapshot_download\n",
        "# snapshot_download(repo_id=\"ybelkada/im2latex-100k\", local_dir=\"./data/hme100k\", repo_type=\"dataset\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_AnwuIUmX1Ls"
      },
      "source": [
        "## Model Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wQx6MytFX1Ls",
        "outputId": "f19eb7ca-7c53-4ca8-fadc-9d8f1fe602ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "GPU: NVIDIA A100-SXM4-80GB\n",
            "Memory: 85.2 GB\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import logging\n",
        "\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Using device: {device}')\n",
        "if torch.cuda.is_available():\n",
        "    print(f'GPU: {torch.cuda.get_device_name(0)}')\n",
        "    print(f'Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xyNHNXC7X1Ls",
        "outputId": "66aeb9ba-b6cd-449d-b497-35c3675020da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenizer vocab size: 1294\n"
          ]
        }
      ],
      "source": [
        "from models import HandwrittenLaTeXOCR, ModelConfig\n",
        "from models.decoder.tokenizer import LaTeXTokenizer\n",
        "from data import DatasetConfig, CombinedDataset, get_train_transforms, get_eval_transforms\n",
        "from training import Trainer, TrainingConfig\n",
        "\n",
        "tokenizer = LaTeXTokenizer()\n",
        "print(f'Tokenizer vocab size: {tokenizer.vocab_size}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JpwbBUn9X1Ls",
        "outputId": "55ad095f-ad45-4bb2-b8d1-6c1152627d44"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model parameters: 15,195,406\n"
          ]
        }
      ],
      "source": [
        "# Model configuration\n",
        "# Use 'small' for testing, 'base' for real training\n",
        "ENCODER = 'fastvithd'\n",
        "ENCODER_SIZE = 'base'  # Change to 'base' for full training\n",
        "\n",
        "model_config = ModelConfig(\n",
        "    encoder_type=ENCODER,\n",
        "    encoder_size=ENCODER_SIZE,\n",
        "    image_size=384,\n",
        "    d_model=256 if ENCODER_SIZE == 'small' else 384,\n",
        "    num_decoder_layers=4 if ENCODER_SIZE == 'small' else 6,\n",
        "    freeze_encoder=True,\n",
        ")\n",
        "\n",
        "model = HandwrittenLaTeXOCR(model_config)\n",
        "print(f'Model parameters: {model.count_parameters():,}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MCunEgrEX1Ls",
        "outputId": "7cfb7d50-2f9a-4470-fa30-8858df3fe666"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Available datasets: ['mathwriting']\n",
            "Train samples: 229864\n",
            "Val samples: 15674\n"
          ]
        }
      ],
      "source": [
        "# Dataset configuration\n",
        "dataset_config = DatasetConfig(data_dir='./data', image_size=384)\n",
        "\n",
        "train_transform = get_train_transforms(image_size=384, augment_strength='medium')\n",
        "valid_transform = get_eval_transforms(image_size=384)\n",
        "\n",
        "# Check available datasets\n",
        "import os\n",
        "available_datasets = []\n",
        "for ds in ['mathwriting', 'crohme', 'hme100k']:\n",
        "    if os.path.exists(f'./data/{ds}'):\n",
        "        available_datasets.append(ds)\n",
        "\n",
        "print(f'Available datasets: {available_datasets}')\n",
        "\n",
        "if not available_datasets:\n",
        "    raise RuntimeError('No datasets found! Run the dataset setup cells above first.')\n",
        "\n",
        "train_dataset = CombinedDataset(\n",
        "    dataset_config, split='train', transform=train_transform,\n",
        "    tokenizer=tokenizer, datasets=available_datasets\n",
        ")\n",
        "val_dataset = CombinedDataset(\n",
        "    dataset_config, split='val', transform=valid_transform,\n",
        "    tokenizer=tokenizer, datasets=available_datasets\n",
        ")\n",
        "\n",
        "print(f'Train samples: {len(train_dataset)}')\n",
        "print(f'Val samples: {len(val_dataset)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UidZmqAeX1Ls",
        "outputId": "f7b8f133-7502-4238-a7be-fbbe7967aa5a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch size: 96\n",
            "Epochs: 20\n"
          ]
        }
      ],
      "source": [
        "# Training configuration\n",
        "if torch.cuda.is_available():\n",
        "    gpu_memory = torch.cuda.get_device_properties(0).total_memory\n",
        "    BATCH_SIZE = 96 if gpu_memory > 70e9 else (32 if gpu_memory > 40e9 else 16)\n",
        "else:\n",
        "    BATCH_SIZE = 4\n",
        "\n",
        "training_config = TrainingConfig(\n",
        "    output_dir='/content/drive/MyDrive/latex_ocr_outputs',\n",
        "    experiment_name=f'latex_ocr_{ENCODER}_{ENCODER_SIZE}',\n",
        "    num_epochs=5 if len(train_dataset) < 1000 else 20,  # Fewer epochs for dummy data\n",
        "    batch_size=BATCH_SIZE,\n",
        "    learning_rate=1e-4,\n",
        "    weight_decay=0.01,\n",
        "    warmup_steps=100 if len(train_dataset) < 1000 else 2000,\n",
        "    gradient_accumulation_steps=2,\n",
        "    use_amp=True,\n",
        "    amp_dtype='bfloat16' if torch.cuda.is_available() and torch.cuda.get_device_capability()[0] >= 8 else 'float16',\n",
        "    save_steps=500,\n",
        "    validation_steps=100 if len(train_dataset) < 1000 else 1000,\n",
        "    log_steps=10 if len(train_dataset) < 1000 else 100,\n",
        "    freeze_encoder_epochs=1,\n",
        "    early_stopping_patience=5,\n",
        ")\n",
        "\n",
        "print(f'Batch size: {BATCH_SIZE}')\n",
        "print(f'Epochs: {training_config.num_epochs}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SAPZrpdbX1Ls",
        "outputId": "33575a09-e878-42d9-baa1-2af3274558f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train batches: 2395\n",
            "Val batches: 164\n"
          ]
        }
      ],
      "source": [
        "# Create dataloaders\n",
        "train_loader = train_dataset.get_dataloader(\n",
        "    batch_size=BATCH_SIZE,\n",
        "    num_workers=2,\n",
        "    use_weighted_sampling=len(train_dataset) > 0\n",
        ")\n",
        "val_loader = val_dataset.get_dataloader(\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=2,\n",
        "    use_weighted_sampling=False\n",
        ")\n",
        "\n",
        "print(f'Train batches: {len(train_loader)}')\n",
        "print(f'Val batches: {len(val_loader)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FmTSCI3xX1Ls"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_nd7051_afoA"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RwJrvyBWX1Ls",
        "outputId": "f14c3aa5-1d40-400e-d5f0-ce5c73f53e71"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resuming from /content/drive/MyDrive/latex_ocr_outputs/latex_ocr_fastvithd_base/checkpoints/step_15500.pt\n",
            "Loaded checkpoint: epoch 5, step 15500, best_metric 0.0022\n",
            "Starting training...\n",
            "Starting training: epochs 5-19, 2396 batches/epoch, step 15500\n",
            "Step 15600: loss=2.2788, lr=9.62e-05\n",
            "Step 15700: loss=2.1537, lr=9.61e-05\n",
            "Step 15800: loss=2.2121, lr=9.60e-05\n",
            "Step 15900: loss=2.0953, lr=9.60e-05\n",
            "Step 16000: loss=2.0737, lr=9.59e-05\n",
            "Step 16100: loss=2.2405, lr=9.58e-05\n",
            "Step 16200: loss=2.2173, lr=9.58e-05\n",
            "Step 16300: loss=2.0796, lr=9.57e-05\n",
            "Step 16400: loss=2.1200, lr=9.56e-05\n",
            "Step 16500: loss=2.2736, lr=9.55e-05\n",
            "Step 16600: loss=2.1988, lr=9.55e-05\n",
            "Step 16700: loss=2.0719, lr=9.54e-05\n",
            "Step 16800: loss=2.1496, lr=9.53e-05\n",
            "Step 16900: loss=2.2009, lr=9.53e-05\n",
            "Step 17000: loss=2.1744, lr=9.52e-05\n",
            "Step 17100: loss=2.2083, lr=9.51e-05\n",
            "Step 17200: loss=2.2555, lr=9.50e-05\n",
            "Step 17300: loss=2.1294, lr=9.50e-05\n",
            "Step 17400: loss=2.1875, lr=9.49e-05\n",
            "Step 17500: loss=2.2445, lr=9.48e-05\n",
            "Step 17600: loss=2.1929, lr=9.47e-05\n",
            "Step 17700: loss=2.1615, lr=9.47e-05\n",
            "Step 17800: loss=2.2426, lr=9.46e-05\n",
            "Epoch 5 train: {'loss': 2.176686747245279}\n",
            "Epoch 5 val: {'exp_rate': 0.0028036192175353637, 'symbol_accuracy': 0.27837069897795536, 'bleu': 0.03111456445130838, 'loss': 2.4832214480493127}\n",
            "Step 17900: loss=2.2080, lr=9.45e-05\n",
            "Step 18000: loss=2.1564, lr=9.44e-05\n",
            "Step 18100: loss=2.1811, lr=9.44e-05\n",
            "Step 18200: loss=2.1866, lr=9.43e-05\n",
            "Step 18300: loss=2.1882, lr=9.42e-05\n",
            "Step 18400: loss=2.2678, lr=9.41e-05\n",
            "Step 18500: loss=2.0878, lr=9.40e-05\n",
            "Step 18600: loss=2.0569, lr=9.40e-05\n",
            "Step 18700: loss=2.1519, lr=9.39e-05\n",
            "Step 18800: loss=2.1888, lr=9.38e-05\n",
            "Step 18900: loss=2.1869, lr=9.37e-05\n",
            "Step 19000: loss=2.1643, lr=9.36e-05\n",
            "Step 19100: loss=2.0123, lr=9.35e-05\n",
            "Step 19200: loss=2.0716, lr=9.35e-05\n",
            "Step 19300: loss=2.0061, lr=9.34e-05\n",
            "Step 19400: loss=2.1546, lr=9.33e-05\n",
            "Step 19500: loss=2.1077, lr=9.32e-05\n",
            "Step 19600: loss=2.1031, lr=9.31e-05\n",
            "Step 19700: loss=2.0599, lr=9.30e-05\n",
            "Step 19800: loss=2.2207, lr=9.29e-05\n",
            "Step 19900: loss=2.1470, lr=9.29e-05\n",
            "Step 20000: loss=2.1674, lr=9.28e-05\n",
            "Step 20100: loss=2.1320, lr=9.27e-05\n",
            "Step 20200: loss=2.0957, lr=9.26e-05\n",
            "Epoch 6 train: {'loss': 2.1372332265958165}\n",
            "Epoch 6 val: {'exp_rate': 0.0037593984962406013, 'symbol_accuracy': 0.287386725785151, 'bleu': 0.0325437433399354, 'loss': 2.4415597653970487}\n",
            "Step 20300: loss=2.1165, lr=9.25e-05\n",
            "Step 20400: loss=2.1211, lr=9.24e-05\n",
            "Step 20500: loss=2.0359, lr=9.23e-05\n",
            "Step 20600: loss=2.1100, lr=9.22e-05\n",
            "Step 20700: loss=1.9408, lr=9.21e-05\n",
            "Step 20800: loss=2.1874, lr=9.20e-05\n",
            "Step 20900: loss=1.9716, lr=9.20e-05\n",
            "Step 21000: loss=2.0678, lr=9.19e-05\n",
            "Step 21100: loss=2.1899, lr=9.18e-05\n",
            "Step 21200: loss=2.1611, lr=9.17e-05\n",
            "Step 21300: loss=1.9599, lr=9.16e-05\n",
            "Step 21400: loss=1.9071, lr=9.15e-05\n",
            "Step 21500: loss=2.0503, lr=9.14e-05\n",
            "Step 21600: loss=2.0134, lr=9.13e-05\n",
            "Step 21700: loss=2.1734, lr=9.12e-05\n",
            "Step 21800: loss=2.1312, lr=9.11e-05\n",
            "Step 21900: loss=2.0214, lr=9.10e-05\n",
            "Step 22000: loss=2.1041, lr=9.09e-05\n",
            "Step 22100: loss=2.0305, lr=9.08e-05\n",
            "Step 22200: loss=2.0921, lr=9.07e-05\n",
            "Step 22300: loss=2.0735, lr=9.06e-05\n",
            "Step 22400: loss=2.0309, lr=9.05e-05\n",
            "Step 22500: loss=2.0974, lr=9.04e-05\n",
            "Step 22600: loss=2.0431, lr=9.03e-05\n",
            "Epoch 7 train: {'loss': 2.0884916236583697}\n",
            "Epoch 7 val: {'exp_rate': 0.004077991589142347, 'symbol_accuracy': 0.2853033197979396, 'bleu': 0.03302742591650149, 'loss': 2.4006405559981743}\n",
            "Step 22700: loss=1.8806, lr=9.02e-05\n",
            "Step 22800: loss=1.9492, lr=9.01e-05\n",
            "Step 22900: loss=2.0890, lr=9.00e-05\n",
            "Step 23000: loss=2.0392, lr=8.99e-05\n",
            "Step 23100: loss=2.1888, lr=8.98e-05\n",
            "Step 23200: loss=2.1528, lr=8.97e-05\n",
            "Step 23300: loss=1.9977, lr=8.96e-05\n",
            "Step 23400: loss=2.0811, lr=8.95e-05\n",
            "Step 23500: loss=2.0779, lr=8.94e-05\n",
            "Step 23600: loss=2.1563, lr=8.93e-05\n",
            "Step 23700: loss=2.1094, lr=8.92e-05\n",
            "Step 23800: loss=2.1308, lr=8.91e-05\n",
            "Step 23900: loss=2.0607, lr=8.90e-05\n",
            "Step 24000: loss=2.1768, lr=8.89e-05\n",
            "Step 24100: loss=2.1615, lr=8.88e-05\n",
            "Step 24200: loss=2.0219, lr=8.86e-05\n",
            "Step 24300: loss=2.1101, lr=8.85e-05\n",
            "Step 24400: loss=2.1345, lr=8.84e-05\n",
            "Step 24500: loss=2.2271, lr=8.83e-05\n",
            "Step 24600: loss=2.0088, lr=8.82e-05\n",
            "Step 24700: loss=2.0459, lr=8.81e-05\n",
            "Step 24800: loss=1.9303, lr=8.80e-05\n",
            "Step 24900: loss=2.1086, lr=8.79e-05\n",
            "Step 25000: loss=2.0344, lr=8.78e-05\n",
            "Epoch 8 train: {'loss': 2.0516085678926097}\n",
            "Epoch 8 val: {'exp_rate': 0.004269147444883395, 'symbol_accuracy': 0.29119335859032347, 'bleu': 0.03360820125373064, 'loss': 2.357775265123786}\n",
            "Step 25100: loss=1.8940, lr=8.77e-05\n",
            "Step 25200: loss=2.0171, lr=8.75e-05\n",
            "Step 25300: loss=2.0457, lr=8.74e-05\n",
            "Step 25400: loss=1.9764, lr=8.73e-05\n",
            "Step 25500: loss=1.9251, lr=8.72e-05\n",
            "Step 25600: loss=2.0706, lr=8.71e-05\n",
            "Step 25700: loss=2.0453, lr=8.70e-05\n",
            "Step 25800: loss=1.9652, lr=8.69e-05\n",
            "Step 25900: loss=1.9696, lr=8.67e-05\n",
            "Step 26000: loss=2.0543, lr=8.66e-05\n",
            "Step 26100: loss=2.1309, lr=8.65e-05\n",
            "Step 26200: loss=1.9834, lr=8.64e-05\n",
            "Step 26300: loss=2.2008, lr=8.63e-05\n",
            "Step 26400: loss=1.9364, lr=8.62e-05\n",
            "Step 26500: loss=2.0095, lr=8.60e-05\n",
            "Step 26600: loss=1.9239, lr=8.59e-05\n",
            "Step 26700: loss=2.0536, lr=8.58e-05\n",
            "Step 26800: loss=1.9683, lr=8.57e-05\n",
            "Step 26900: loss=2.1677, lr=8.56e-05\n",
            "Step 27000: loss=2.0409, lr=8.55e-05\n",
            "Step 27100: loss=1.8896, lr=8.53e-05\n",
            "Step 27200: loss=2.0346, lr=8.52e-05\n",
            "Step 27300: loss=2.0967, lr=8.51e-05\n",
            "Step 27400: loss=1.9669, lr=8.50e-05\n",
            "Epoch 9 train: {'loss': 2.0147046190569915}\n",
            "Epoch 9 val: {'exp_rate': 0.005543519816490378, 'symbol_accuracy': 0.29793938117597935, 'bleu': 0.03505684915654417, 'loss': 2.33121441341028}\n",
            "Step 27500: loss=1.9644, lr=8.48e-05\n",
            "Step 27600: loss=2.0810, lr=8.47e-05\n",
            "Step 27700: loss=2.0585, lr=8.46e-05\n",
            "Step 27800: loss=2.0180, lr=8.45e-05\n",
            "Step 27900: loss=2.0051, lr=8.44e-05\n",
            "Step 28000: loss=1.9924, lr=8.42e-05\n",
            "Step 28100: loss=2.0907, lr=8.41e-05\n",
            "Step 28200: loss=2.1665, lr=8.40e-05\n",
            "Step 28300: loss=1.9972, lr=8.39e-05\n",
            "Step 28400: loss=1.9228, lr=8.37e-05\n",
            "Step 28500: loss=1.9984, lr=8.36e-05\n",
            "Step 28600: loss=2.0951, lr=8.35e-05\n",
            "Step 28700: loss=2.0021, lr=8.34e-05\n",
            "Step 28800: loss=1.8672, lr=8.32e-05\n",
            "Step 28900: loss=1.9995, lr=8.31e-05\n",
            "Step 29000: loss=2.0655, lr=8.30e-05\n",
            "Step 29100: loss=2.0513, lr=8.28e-05\n",
            "Step 29200: loss=2.0148, lr=8.27e-05\n",
            "Step 29300: loss=2.0227, lr=8.26e-05\n",
            "Step 29400: loss=2.0374, lr=8.25e-05\n",
            "Step 29500: loss=1.9123, lr=8.23e-05\n",
            "Step 29600: loss=1.8992, lr=8.22e-05\n",
            "Step 29700: loss=1.9951, lr=8.21e-05\n",
            "Step 29800: loss=1.9053, lr=8.19e-05\n",
            "Epoch 10 train: {'loss': 1.9799783395506902}\n",
            "Epoch 10 val: {'exp_rate': 0.00618070600229387, 'symbol_accuracy': 0.29525972596823846, 'bleu': 0.03605089274752459, 'loss': 2.306680269357635}\n",
            "Step 29900: loss=2.0659, lr=8.18e-05\n",
            "Step 30000: loss=1.9079, lr=8.17e-05\n",
            "Step 30100: loss=1.9651, lr=8.15e-05\n",
            "Step 30200: loss=2.0113, lr=8.14e-05\n",
            "Step 30300: loss=1.8270, lr=8.13e-05\n",
            "Step 30400: loss=2.1276, lr=8.12e-05\n",
            "Step 30500: loss=2.1360, lr=8.10e-05\n",
            "Step 30600: loss=1.9607, lr=8.09e-05\n",
            "Step 30700: loss=1.9910, lr=8.07e-05\n",
            "Step 30800: loss=2.0007, lr=8.06e-05\n",
            "Step 30900: loss=1.9858, lr=8.05e-05\n",
            "Step 31000: loss=1.9383, lr=8.03e-05\n",
            "Step 31100: loss=1.8131, lr=8.02e-05\n",
            "Step 31200: loss=2.0199, lr=8.01e-05\n",
            "Step 31300: loss=2.0296, lr=7.99e-05\n",
            "Step 31400: loss=1.9534, lr=7.98e-05\n",
            "Step 31500: loss=1.9256, lr=7.97e-05\n",
            "Step 31600: loss=1.9569, lr=7.95e-05\n",
            "Step 31700: loss=1.9012, lr=7.94e-05\n",
            "Step 31800: loss=1.9328, lr=7.93e-05\n",
            "Step 31900: loss=1.9830, lr=7.91e-05\n",
            "Step 32000: loss=1.8725, lr=7.90e-05\n",
            "Step 32100: loss=1.9857, lr=7.88e-05\n",
            "Step 32200: loss=1.9446, lr=7.87e-05\n",
            "Epoch 11 train: {'loss': 1.9483852413341478}\n",
            "Epoch 11 val: {'exp_rate': 0.007518796992481203, 'symbol_accuracy': 0.30131705932265856, 'bleu': 0.037702438066923936, 'loss': 2.278437656600301}\n",
            "Step 32300: loss=1.9214, lr=7.86e-05\n",
            "Step 32400: loss=2.0173, lr=7.84e-05\n",
            "Step 32500: loss=1.8976, lr=7.83e-05\n",
            "Step 32600: loss=2.0690, lr=7.81e-05\n",
            "Step 32700: loss=1.8593, lr=7.80e-05\n",
            "Step 32800: loss=1.8992, lr=7.79e-05\n",
            "Step 32900: loss=1.8618, lr=7.77e-05\n",
            "Step 33000: loss=1.8305, lr=7.76e-05\n",
            "Step 33100: loss=1.9958, lr=7.74e-05\n",
            "Step 33200: loss=1.9402, lr=7.73e-05\n",
            "Step 33300: loss=1.9389, lr=7.72e-05\n",
            "Step 33400: loss=1.7950, lr=7.70e-05\n",
            "Step 33500: loss=1.9178, lr=7.69e-05\n",
            "Step 33600: loss=1.8705, lr=7.67e-05\n",
            "Step 33700: loss=1.9020, lr=7.66e-05\n",
            "Step 33800: loss=1.9339, lr=7.64e-05\n",
            "Step 33900: loss=2.0126, lr=7.63e-05\n",
            "Step 34000: loss=1.9124, lr=7.61e-05\n",
            "Step 34100: loss=1.8843, lr=7.60e-05\n",
            "Step 34200: loss=1.8541, lr=7.59e-05\n",
            "Step 34300: loss=1.9719, lr=7.57e-05\n",
            "Step 34400: loss=1.9537, lr=7.56e-05\n",
            "Step 34500: loss=1.9182, lr=7.54e-05\n",
            "Step 34600: loss=2.0950, lr=7.53e-05\n",
            "Epoch 12 train: {'loss': 1.921582930573637}\n",
            "Epoch 12 val: {'exp_rate': 0.00853829488976679, 'symbol_accuracy': 0.3007842392723877, 'bleu': 0.03833982962068841, 'loss': 2.267137101510676}\n",
            "Step 34700: loss=2.0044, lr=7.51e-05\n",
            "Step 34800: loss=1.8098, lr=7.50e-05\n",
            "Step 34900: loss=1.8603, lr=7.48e-05\n",
            "Step 35000: loss=1.9899, lr=7.47e-05\n",
            "Step 35100: loss=1.9866, lr=7.45e-05\n",
            "Step 35200: loss=1.8479, lr=7.44e-05\n",
            "Step 35300: loss=2.0064, lr=7.42e-05\n",
            "Step 35400: loss=1.9010, lr=7.41e-05\n",
            "Step 35500: loss=1.8359, lr=7.39e-05\n",
            "Step 35600: loss=1.9639, lr=7.38e-05\n",
            "Step 35700: loss=1.9376, lr=7.36e-05\n",
            "Step 35800: loss=1.8579, lr=7.35e-05\n",
            "Step 35900: loss=2.0391, lr=7.33e-05\n",
            "Step 36000: loss=2.0002, lr=7.32e-05\n",
            "Step 36100: loss=1.7681, lr=7.30e-05\n",
            "Step 36200: loss=1.8895, lr=7.29e-05\n",
            "Step 36300: loss=2.0362, lr=7.27e-05\n",
            "Step 36400: loss=1.9229, lr=7.26e-05\n",
            "Step 36500: loss=1.8375, lr=7.24e-05\n",
            "Step 36600: loss=1.7791, lr=7.23e-05\n",
            "Step 36700: loss=1.8794, lr=7.21e-05\n",
            "Step 36800: loss=1.8279, lr=7.20e-05\n",
            "Step 36900: loss=1.8922, lr=7.18e-05\n",
            "Step 37000: loss=1.7852, lr=7.17e-05\n",
            "Epoch 13 train: {'loss': 1.898777906653877}\n",
            "Epoch 13 val: {'exp_rate': 0.009494074168472028, 'symbol_accuracy': 0.3058803692005133, 'bleu': 0.04137887903816104, 'loss': 2.244683553532856}\n",
            "Step 37100: loss=1.9122, lr=7.15e-05\n",
            "Step 37200: loss=1.9556, lr=7.14e-05\n",
            "Step 37300: loss=1.8340, lr=7.12e-05\n",
            "Step 37400: loss=1.8744, lr=7.11e-05\n",
            "Step 37500: loss=1.8535, lr=7.09e-05\n",
            "Step 37600: loss=1.7744, lr=7.07e-05\n",
            "Step 37700: loss=1.8278, lr=7.06e-05\n",
            "Step 37800: loss=2.1203, lr=7.04e-05\n",
            "Step 37900: loss=1.9399, lr=7.03e-05\n",
            "Step 38000: loss=1.8812, lr=7.01e-05\n",
            "Step 38100: loss=1.9932, lr=7.00e-05\n",
            "Step 38200: loss=2.0050, lr=6.98e-05\n",
            "Step 38300: loss=1.9556, lr=6.97e-05\n",
            "Step 38400: loss=1.8764, lr=6.95e-05\n",
            "Step 38500: loss=1.8771, lr=6.93e-05\n",
            "Step 38600: loss=1.9074, lr=6.92e-05\n",
            "Step 38700: loss=1.8097, lr=6.90e-05\n",
            "Step 38800: loss=1.8043, lr=6.89e-05\n",
            "Step 38900: loss=1.8863, lr=6.87e-05\n",
            "Step 39000: loss=2.0443, lr=6.86e-05\n",
            "Step 39100: loss=1.8033, lr=6.84e-05\n",
            "Step 39200: loss=1.8068, lr=6.82e-05\n",
            "Step 39300: loss=1.8895, lr=6.81e-05\n",
            "Step 39400: loss=1.8703, lr=6.79e-05\n",
            "Epoch 14 train: {'loss': 1.8820201880943794}\n",
            "Epoch 14 val: {'exp_rate': 0.011469351344462851, 'symbol_accuracy': 0.3107207555831973, 'bleu': 0.04110644743261256, 'loss': 2.230964621392692}\n",
            "Step 39500: loss=1.8455, lr=6.78e-05\n",
            "Step 39600: loss=1.8267, lr=6.76e-05\n",
            "Step 39700: loss=1.8343, lr=6.75e-05\n",
            "Step 39800: loss=1.8700, lr=6.73e-05\n",
            "Step 39900: loss=1.8553, lr=6.71e-05\n",
            "Step 40000: loss=1.9504, lr=6.70e-05\n",
            "Step 40100: loss=1.9193, lr=6.68e-05\n",
            "Step 40200: loss=1.8497, lr=6.67e-05\n",
            "Step 40300: loss=1.8342, lr=6.65e-05\n",
            "Step 40400: loss=1.7717, lr=6.63e-05\n",
            "Step 40500: loss=1.9731, lr=6.62e-05\n",
            "Step 40600: loss=1.8968, lr=6.60e-05\n",
            "Step 40700: loss=1.7836, lr=6.59e-05\n",
            "Step 40800: loss=1.7431, lr=6.57e-05\n",
            "Step 40900: loss=1.8050, lr=6.55e-05\n",
            "Step 41000: loss=1.9522, lr=6.54e-05\n",
            "Step 41100: loss=1.7245, lr=6.52e-05\n",
            "Step 41200: loss=1.8953, lr=6.50e-05\n",
            "Step 41300: loss=1.8842, lr=6.49e-05\n",
            "Step 41400: loss=1.8541, lr=6.47e-05\n",
            "Step 41500: loss=1.8195, lr=6.46e-05\n",
            "Step 41600: loss=1.9093, lr=6.44e-05\n",
            "Step 41700: loss=1.8075, lr=6.42e-05\n",
            "Step 41800: loss=1.7880, lr=6.41e-05\n",
            "Epoch 15 train: {'loss': 1.8635501009793034}\n",
            "Epoch 15 val: {'exp_rate': 0.01312603542755193, 'symbol_accuracy': 0.3128948512233746, 'bleu': 0.04418627742631035, 'loss': 2.2139241331961097}\n",
            "Step 41900: loss=1.9316, lr=6.39e-05\n",
            "Step 42000: loss=1.7655, lr=6.37e-05\n",
            "Step 42100: loss=1.9468, lr=6.36e-05\n",
            "Step 42200: loss=1.8190, lr=6.34e-05\n",
            "Step 42300: loss=1.8725, lr=6.33e-05\n",
            "Step 42400: loss=1.8737, lr=6.31e-05\n",
            "Step 42500: loss=1.9749, lr=6.29e-05\n",
            "Step 42600: loss=1.9662, lr=6.28e-05\n",
            "Step 42700: loss=1.8436, lr=6.26e-05\n",
            "Step 42800: loss=1.8388, lr=6.24e-05\n",
            "Step 42900: loss=1.7622, lr=6.23e-05\n",
            "Step 43000: loss=1.7619, lr=6.21e-05\n",
            "Step 43100: loss=1.7962, lr=6.19e-05\n",
            "Step 43200: loss=1.6816, lr=6.18e-05\n",
            "Step 43300: loss=1.9349, lr=6.16e-05\n",
            "Step 43400: loss=1.9704, lr=6.14e-05\n",
            "Step 43500: loss=1.7320, lr=6.13e-05\n",
            "Step 43600: loss=2.0222, lr=6.11e-05\n",
            "Step 43700: loss=1.7264, lr=6.10e-05\n",
            "Step 43800: loss=1.7179, lr=6.08e-05\n",
            "Step 43900: loss=1.8654, lr=6.06e-05\n",
            "Step 44000: loss=1.8602, lr=6.05e-05\n",
            "Step 44100: loss=1.7425, lr=6.03e-05\n",
            "Step 44200: loss=1.8981, lr=6.01e-05\n",
            "Epoch 16 train: {'loss': 1.8502221902543197}\n",
            "Epoch 16 val: {'exp_rate': 0.013890658850516121, 'symbol_accuracy': 0.31504469192784657, 'bleu': 0.044960211628461304, 'loss': 2.2095439564890977}\n",
            "Step 44300: loss=1.8957, lr=6.00e-05\n",
            "Step 44400: loss=1.7137, lr=5.98e-05\n",
            "Step 44500: loss=1.8504, lr=5.96e-05\n",
            "Step 44600: loss=1.9442, lr=5.95e-05\n",
            "Step 44700: loss=1.8434, lr=5.93e-05\n",
            "Step 44800: loss=1.9092, lr=5.91e-05\n",
            "Step 44900: loss=1.7823, lr=5.90e-05\n",
            "Step 45000: loss=1.7396, lr=5.88e-05\n",
            "Step 45100: loss=1.8974, lr=5.86e-05\n",
            "Step 45200: loss=1.8631, lr=5.85e-05\n",
            "Step 45300: loss=1.9464, lr=5.83e-05\n",
            "Step 45400: loss=1.9220, lr=5.81e-05\n",
            "Step 45500: loss=1.6570, lr=5.80e-05\n",
            "Step 45600: loss=1.7163, lr=5.78e-05\n",
            "Step 45700: loss=1.8317, lr=5.76e-05\n",
            "Step 45800: loss=1.7230, lr=5.75e-05\n",
            "Step 45900: loss=1.8341, lr=5.73e-05\n",
            "Step 46000: loss=1.7462, lr=5.71e-05\n",
            "Step 46100: loss=1.8051, lr=5.69e-05\n",
            "Step 46200: loss=1.9244, lr=5.68e-05\n",
            "Step 46300: loss=1.7303, lr=5.66e-05\n",
            "Step 46400: loss=1.7407, lr=5.64e-05\n",
            "Step 46500: loss=1.8822, lr=5.63e-05\n",
            "Step 46600: loss=1.7302, lr=5.61e-05\n",
            "Epoch 17 train: {'loss': 1.8332609736760193}\n",
            "Epoch 17 val: {'exp_rate': 0.014209251943417867, 'symbol_accuracy': 0.31864296743654974, 'bleu': 0.043824586310407275, 'loss': 2.200789954604172}\n",
            "Step 46700: loss=1.8651, lr=5.59e-05\n",
            "Step 46800: loss=1.9344, lr=5.58e-05\n",
            "Step 46900: loss=1.7761, lr=5.56e-05\n",
            "Step 47000: loss=1.7381, lr=5.54e-05\n",
            "Step 47100: loss=1.7630, lr=5.53e-05\n",
            "Step 47200: loss=1.7571, lr=5.51e-05\n",
            "Step 47300: loss=1.8605, lr=5.49e-05\n",
            "Step 47400: loss=1.7870, lr=5.48e-05\n",
            "Step 47500: loss=1.6890, lr=5.46e-05\n",
            "Step 47600: loss=1.7176, lr=5.44e-05\n",
            "Step 47700: loss=1.7480, lr=5.43e-05\n",
            "Step 47800: loss=1.6829, lr=5.41e-05\n",
            "Step 47900: loss=1.8295, lr=5.39e-05\n",
            "Step 48000: loss=1.8404, lr=5.37e-05\n",
            "Step 48100: loss=1.7850, lr=5.36e-05\n",
            "Step 48200: loss=1.7905, lr=5.34e-05\n",
            "Step 48300: loss=1.8557, lr=5.32e-05\n",
            "Step 48400: loss=1.9115, lr=5.31e-05\n",
            "Step 48500: loss=1.8232, lr=5.29e-05\n",
            "Step 48600: loss=1.8505, lr=5.27e-05\n",
            "Step 48700: loss=1.7101, lr=5.26e-05\n",
            "Step 48800: loss=1.7201, lr=5.24e-05\n",
            "Step 48900: loss=1.6974, lr=5.22e-05\n",
            "Step 49000: loss=1.7691, lr=5.21e-05\n",
            "Epoch 18 train: {'loss': 1.8197567373862449}\n",
            "Epoch 18 val: {'exp_rate': 0.01707658977953358, 'symbol_accuracy': 0.32126422758338663, 'bleu': 0.046446794429536295, 'loss': 2.1844082881764666}\n",
            "Step 49100: loss=1.8120, lr=5.19e-05\n",
            "Step 49200: loss=1.6996, lr=5.17e-05\n",
            "Step 49300: loss=1.7012, lr=5.15e-05\n",
            "Step 49400: loss=1.6916, lr=5.14e-05\n",
            "Step 49500: loss=1.8256, lr=5.12e-05\n",
            "Step 49600: loss=1.7715, lr=5.10e-05\n",
            "Step 49700: loss=1.8525, lr=5.09e-05\n",
            "Step 49800: loss=1.8126, lr=5.07e-05\n",
            "Step 49900: loss=1.7927, lr=5.05e-05\n",
            "Step 50000: loss=1.8689, lr=5.04e-05\n",
            "Step 50100: loss=1.8600, lr=5.02e-05\n",
            "Step 50200: loss=1.6937, lr=5.00e-05\n",
            "Step 50300: loss=1.8405, lr=4.99e-05\n",
            "Step 50400: loss=1.8579, lr=4.97e-05\n",
            "Step 50500: loss=1.7699, lr=4.95e-05\n",
            "Step 50600: loss=1.7452, lr=4.93e-05\n",
            "Step 50700: loss=1.6435, lr=4.92e-05\n",
            "Step 50800: loss=1.8576, lr=4.90e-05\n",
            "Step 50900: loss=1.8132, lr=4.88e-05\n",
            "Step 51000: loss=1.8017, lr=4.87e-05\n",
            "Step 51100: loss=1.9431, lr=4.85e-05\n",
            "Step 51200: loss=1.7945, lr=4.83e-05\n",
            "Step 51300: loss=1.9557, lr=4.82e-05\n",
            "Step 51400: loss=1.8068, lr=4.80e-05\n",
            "Epoch 19 train: {'loss': 1.8093748517048378}\n",
            "Epoch 19 val: {'exp_rate': 0.017458901491015675, 'symbol_accuracy': 0.3202204744828703, 'bleu': 0.045877562378526936, 'loss': 2.176202771140308}\n",
            "Training complete! Best metric: 0.0175\n",
            "Training complete! Best metric: 0.0175\n"
          ]
        }
      ],
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    train_loader=train_loader,\n",
        "    val_loader=val_loader,\n",
        "    config=training_config,\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "# Resume from latest checkpoint\n",
        "import glob\n",
        "checkpoints = glob.glob('/content/drive/MyDrive/latex_ocr_outputs/latex_ocr_fastvithd_base/checkpoints/step_*.pt')\n",
        "if checkpoints:\n",
        "    latest = max(checkpoints, key=lambda x:\n",
        "int(x.split('_')[-1].split('.')[0]))\n",
        "    print(f'Resuming from {latest}')\n",
        "    trainer.load_checkpoint(latest)\n",
        "\n",
        "print('Starting training...')\n",
        "best_metric = trainer.train()\n",
        "print(f'Training complete! Best metric: {best_metric:.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "jP5300XsPqzk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "a6V3dUENX1Ls",
        "outputId": "d12c26dc-38f4-467b-dcc8-feb08e8f4796",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved model to /content/drive/MyDrive/latex_ocr_outputs/final_model\n"
          ]
        }
      ],
      "source": [
        "# Save final model\n",
        "save_path = training_config.output_dir + '/final_model'\n",
        "model.save_pretrained(save_path)\n",
        "print(f'Saved model to {save_path}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0bmVm6LlX1Ls"
      },
      "source": [
        "## Test Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "aXvtcXS4X1Ls",
        "outputId": "f95e6954-2a50-4de4-d1e7-3502627bfdf4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ground truth: r=g^{e}y(\\mod p)\n",
            "Predicted: m(x)=\\int_{1}^{x}\\frac{1}{t}dt\n"
          ]
        }
      ],
      "source": [
        "# Test on a sample\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    # Get a sample from validation set\n",
        "    sample = val_dataset[0]\n",
        "    img = sample.image.unsqueeze(0).to(device)\n",
        "\n",
        "    output = model(img)\n",
        "\n",
        "    print(f'Ground truth: {sample.latex}')\n",
        "    if output.predictions and output.predictions[0]:\n",
        "        pred_latex = output.predictions[0][0][1] if output.predictions[0][0] else ''\n",
        "        print(f'Predicted: {pred_latex}')\n",
        "    else:\n",
        "        print('No prediction generated')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Compare Greedy and Beam Search"
      ],
      "metadata": {
        "id": "Ek_eurTrC8zP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "from training.metrics import compute_metrics\n",
        "\n",
        "model.eval()\n",
        "device = torch.device('cuda')\n",
        "model = model.to(device)\n",
        "\n",
        "def run_validation(model, val_loader, use_beam=False, beam_size=5):\n",
        "    all_predictions = []\n",
        "    all_targets = []\n",
        "\n",
        "    for batch in tqdm(val_loader, desc=f\"{'Beam' if use_beam else 'Greedy'} eval\"):\n",
        "        images = batch['images'].to(device)\n",
        "        targets = batch['latex']\n",
        "\n",
        "        with torch.no_grad():\n",
        "            if use_beam:\n",
        "                output = model.beam_search(images, beam_size=beam_size)\n",
        "            else:\n",
        "                output = model(images)  # greedy\n",
        "\n",
        "        for pred_regions in output.predictions:\n",
        "            if pred_regions:\n",
        "                pred_latex = pred_regions[0][1] if pred_regions[0] else \"\"\n",
        "            else:\n",
        "                pred_latex = \"\"\n",
        "            all_predictions.append(pred_latex)\n",
        "\n",
        "        all_targets.extend(targets)\n",
        "\n",
        "    return compute_metrics(all_predictions, all_targets)\n",
        "\n",
        "  # Run both\n",
        "print(\"Running greedy validation...\")\n",
        "greedy_metrics = run_validation(model, val_loader, use_beam=False)\n",
        "print(f\"Greedy: {greedy_metrics}\")\n",
        "\n",
        "print(\"\\nRunning beam search validation (k=5)...\")\n",
        "beam_metrics = run_validation(model, val_loader, use_beam=True, beam_size=5)\n",
        "print(f\"Beam:   {beam_metrics}\")\n",
        "\n",
        "  # Compare\n",
        "print(\"\\n=== Comparison ===\")\n",
        "for key in greedy_metrics:\n",
        "    diff = beam_metrics[key] - greedy_metrics[key]\n",
        "    print(f\"{key}: {greedy_metrics[key]:.4f} -> {beam_metrics[key]:.4f} ({'+'\n",
        "if diff > 0 else ''}{diff:.4f})\")"
      ],
      "metadata": {
        "id": "SkXXhZ9PAvwI",
        "outputId": "577acee7-0210-4eda-c347-9881d0562278",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running greedy validation...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Greedy eval: 100%|| 164/164 [25:32<00:00,  9.34s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Greedy: {'exp_rate': 0.0, 'symbol_accuracy': 0.021990507751278983, 'bleu': 0.0}\n",
            "\n",
            "Running beam search validation (k=5)...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Beam eval:   5%|         | 8/164 [25:31<8:17:21, 191.29s/it]"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "H100",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}