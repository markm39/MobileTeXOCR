{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# MobileTeXOCR: Train HME Recognition on Google Colab\n",
        "\n",
        "This notebook trains the Handwritten Mathematical Expression (HME) recognition model.\n",
        "\n",
        "**Before running:**\n",
        "1. Go to Runtime → Change runtime type → Select **T4 GPU** (or better)\n",
        "2. Run all cells in order\n",
        "\n",
        "**Model Variants:**\n",
        "| Variant | Training Time | Model Size | Expected Accuracy |\n",
        "|---------|--------------|------------|-------------------|\n",
        "| Ultra-light | ~2-3 hours | <10MB | ~55% ExpRate |\n",
        "| Balanced | ~6-8 hours | 15-30MB | ~62% ExpRate |\n",
        "| Accuracy | ~12-16 hours | 50-80MB | ~65% ExpRate |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check GPU availability\n",
        "!nvidia-smi\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Install PaddlePaddle\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install PaddlePaddle GPU version\n",
        "%pip install paddlepaddle-gpu==2.6.1 -i https://pypi.tuna.tsinghua.edu.cn/simple\n",
        "\n",
        "# Verify installation\n",
        "import paddle\n",
        "print(f\"PaddlePaddle version: {paddle.__version__}\")\n",
        "print(f\"GPU available: {paddle.device.is_compiled_with_cuda()}\")\n",
        "print(f\"GPU count: {paddle.device.cuda.device_count()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Clone Repository & Install Dependencies\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Option A: Clone from GitHub (update with your repo URL)\n",
        "# !git clone https://github.com/YOUR_USERNAME/MobileTeXOCR.git\n",
        "# %cd MobileTeXOCR\n",
        "\n",
        "# Option B: Upload zip file from local machine\n",
        "from google.colab import files\n",
        "print(\"Upload MobileTeXOCR.zip (create it with: zip -r MobileTeXOCR.zip MobileTeXOCR/)\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "!unzip -q MobileTeXOCR.zip\n",
        "%cd MobileTeXOCR\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "%pip install -q -r requirements.txt\n",
        "%pip install -q visualdl shapely pyclipper lmdb\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Download CROHME Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python tools/download_hme_datasets.py --dataset crohme --data_dir ./train_data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Select Model Variant & Train\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Choose your model variant: \"ultralight\", \"balanced\", or \"accuracy\"\n",
        "MODEL_VARIANT = \"balanced\"\n",
        "\n",
        "config_map = {\n",
        "    \"ultralight\": \"configs/rec/hme_latex_ocr_ultralight.yml\",\n",
        "    \"balanced\": \"configs/rec/hme_latex_ocr_balanced.yml\",\n",
        "    \"accuracy\": \"configs/rec/hme_latex_ocr_accuracy.yml\",\n",
        "}\n",
        "\n",
        "CONFIG_PATH = config_map[MODEL_VARIANT]\n",
        "print(f\"Training {MODEL_VARIANT} model with config: {CONFIG_PATH}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Start training!\n",
        "!python tools/train.py -c {CONFIG_PATH}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Export & Download Trained Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Find best checkpoint and export\n",
        "import os\n",
        "output_dir = f\"./output/rec/hme_{MODEL_VARIANT}/\"\n",
        "best_ckpt = os.path.join(output_dir, \"best_accuracy\")\n",
        "\n",
        "# Export to inference model\n",
        "!python tools/export_model.py -c {CONFIG_PATH} \\\n",
        "    -o Global.checkpoints={best_ckpt} \\\n",
        "    Global.save_inference_dir=./inference/hme_{MODEL_VARIANT}/\n",
        "\n",
        "# Check model size\n",
        "!du -sh ./inference/hme_{MODEL_VARIANT}/\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Zip and download trained model\n",
        "!zip -r hme_model.zip ./output/rec/hme_{MODEL_VARIANT}/ ./inference/hme_{MODEL_VARIANT}/\n",
        "\n",
        "from google.colab import files\n",
        "files.download('hme_model.zip')\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
