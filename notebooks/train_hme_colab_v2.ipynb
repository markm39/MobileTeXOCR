{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MobileTeXOCR V2: Train HME Recognition on Google Colab\n",
    "\n",
    "This notebook trains the **V2** Handwritten Mathematical Expression (HME) recognition model.\n",
    "\n",
    "## V2 Features:\n",
    "- **Proper autoregressive generation** (fixed SOS/EOS handling)\n",
    "- **Differential Attention** (ICLR 2025) - noise cancellation in attention\n",
    "- **Mixture of Experts FFN** - sparse computation\n",
    "- **Optional PaTH Attention** - Householder transform position encoding\n",
    "\n",
    "**Before running:**\n",
    "1. Go to Runtime -> Change runtime type -> Select **T4 GPU** (or better)\n",
    "2. Run all cells in order\n",
    "\n",
    "**Model Variants:**\n",
    "| Variant | Description | Model Size | Features |\n",
    "|---------|-------------|------------|----------|\n",
    "| ultralight_v2 | Differential Attention + MoE | ~7MB | Best for mobile |\n",
    "| ultralight_v2_path | PaTH Attention + MoE | ~7.5MB | Alternative attention |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install PaddlePaddle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install PaddlePaddle GPU version\n",
    "%pip install -q paddlepaddle-gpu==2.6.1 -i https://pypi.tuna.tsinghua.edu.cn/simple\n",
    "\n",
    "# Verify installation\n",
    "import paddle\n",
    "print(f\"PaddlePaddle version: {paddle.__version__}\")\n",
    "print(f\"GPU available: {paddle.device.is_compiled_with_cuda()}\")\n",
    "print(f\"GPU count: {paddle.device.cuda.device_count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Clone Repository & Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone from GitHub\n",
    "!git clone https://github.com/markm39/MobileTeXOCR.git\n",
    "%cd MobileTeXOCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "%pip install -q -r requirements.txt\n",
    "%pip install -q visualdl shapely pyclipper lmdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Download CROHME Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python tools/download_hme_datasets.py --dataset crohme --data_dir ./train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Select Model Variant & Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose your model variant\n",
    "# Options: \"ultralight_v2\" (recommended), \"ultralight_v2_path\"\n",
    "MODEL_VARIANT = \"ultralight_v2\"\n",
    "\n",
    "config_map = {\n",
    "    # V2 models (recommended - proper autoregressive generation)\n",
    "    \"ultralight_v2\": \"configs/rec/hme_latex_ocr_ultralight_v2.yml\",\n",
    "    \"ultralight_v2_path\": \"configs/rec/hme_latex_ocr_ultralight_v2_path.yml\",\n",
    "    # Legacy V1 models (broken inference - for reference only)\n",
    "    \"ultralight\": \"configs/rec/hme_latex_ocr_ultralight.yml\"\n",
    "}\n",
    "\n",
    "CONFIG_PATH = config_map[MODEL_VARIANT]\n",
    "OUTPUT_DIR = f\"./output/rec/hme_{MODEL_VARIANT}/\"\n",
    "\n",
    "print(f\"Model variant: {MODEL_VARIANT}\")\n",
    "print(f\"Config: {CONFIG_PATH}\")\n",
    "print(f\"Output: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure we have the latest code\n",
    "!git pull origin main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training!\n",
    "!python tools/train.py -c {CONFIG_PATH}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Debug & Verify Training (Optional)\n",
    "\n",
    "Run this cell to check if the model is learning properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile debug_preds_v2.py\n",
    "import paddle\n",
    "import numpy as np\n",
    "import sys\n",
    "import logging\n",
    "import os\n",
    "sys.path.insert(0, '.')\n",
    "\n",
    "from ppocr.modeling.architectures import build_model\n",
    "from ppocr.data import build_dataloader\n",
    "from tools.program import load_config\n",
    "\n",
    "logger = logging.getLogger('ppocr')\n",
    "logger.setLevel(logging.INFO)\n",
    "handler = logging.StreamHandler()\n",
    "logger.addHandler(handler)\n",
    "\n",
    "config_path = os.environ.get('CONFIG_PATH', 'configs/rec/hme_latex_ocr_ultralight_v2.yml')\n",
    "output_dir = os.environ.get('OUTPUT_DIR', './output/rec/hme_ultralight_v2/')\n",
    "\n",
    "print(f\"Loading config: {config_path}\")\n",
    "config = load_config(config_path)\n",
    "model = build_model(config['Architecture'])\n",
    "\n",
    "ckpt_path = os.path.join(output_dir, 'latest.pdparams')\n",
    "print(f\"Loading checkpoint: {ckpt_path}\")\n",
    "state = paddle.load(ckpt_path)\n",
    "model.set_state_dict(state)\n",
    "model.eval()\n",
    "\n",
    "valid_dataloader = build_dataloader(config, 'Eval', None, logger)\n",
    "\n",
    "batch = next(iter(valid_dataloader))\n",
    "batch = [paddle.to_tensor(b) if isinstance(b, np.ndarray) else b for b in batch]\n",
    "\n",
    "images, image_masks, decoder_inputs, decoder_targets, label_masks = batch\n",
    "\n",
    "print(f\"\\nBatch shapes:\")\n",
    "print(f\"  images: {images.shape}\")\n",
    "print(f\"  decoder_inputs: {decoder_inputs.shape}\")\n",
    "print(f\"  decoder_targets: {decoder_targets.shape}\")\n",
    "\n",
    "with paddle.no_grad():\n",
    "    logits = model(images)\n",
    "\n",
    "pred_tokens = logits.argmax(axis=-1).numpy()[0][:20]\n",
    "target_tokens = decoder_targets.numpy()[0][:20]\n",
    "\n",
    "print(f\"\\nPredicted: {pred_tokens.tolist()}\")\n",
    "print(f\"Targets:   {target_tokens.tolist()}\")\n",
    "print(f\"Unique predictions: {len(set(pred_tokens))}\")\n",
    "\n",
    "if len(set(pred_tokens[:10])) <= 2:\n",
    "    print(\"\\n[WARNING] Possible repetition collapse detected!\")\n",
    "else:\n",
    "    print(\"\\n[OK] Model producing diverse tokens\")\n",
    "\n",
    "dict_path = config['Global']['character_dict_path']\n",
    "vocab = ['<eos>', '<sos>']\n",
    "with open(dict_path, 'r') as f:\n",
    "    vocab.extend([line.strip() for line in f])\n",
    "\n",
    "print(f\"\\nDecoded prediction:\")\n",
    "decoded = []\n",
    "for t in pred_tokens:\n",
    "    if t == 0:\n",
    "        break\n",
    "    if t == 1:\n",
    "        continue\n",
    "    if t < len(vocab):\n",
    "        decoded.append(vocab[t])\n",
    "print(' '.join(decoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CONFIG_PATH'] = CONFIG_PATH\n",
    "os.environ['OUTPUT_DIR'] = OUTPUT_DIR\n",
    "!python debug_preds_v2.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Test on Sample Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List some test images\n",
    "!ls ./train_data/CROHME/evaluation/images/ | head -10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a test image\n",
    "from IPython.display import Image, display\n",
    "test_image = './train_data/CROHME/evaluation/images/18_em_10.jpg'\n",
    "display(Image(test_image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run inference on test image\n",
    "!python tools/test_hme_model_v2.py \\\n",
    "    --image ./train_data/CROHME/evaluation/images/18_em_10.jpg \\\n",
    "    --checkpoint {OUTPUT_DIR}/best_accuracy \\\n",
    "    --config {CONFIG_PATH}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Export & Download Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to inference model\n",
    "INFERENCE_DIR = f\"./inference/hme_{MODEL_VARIANT}/\"\n",
    "\n",
    "!python tools/export_model.py -c {CONFIG_PATH} \\\n",
    "    -o Global.export_with_pir=False \\\n",
    "       Global.pretrained_model={OUTPUT_DIR}/best_accuracy \\\n",
    "       Global.save_inference_dir={INFERENCE_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check model size\n",
    "print(\"Model size:\")\n",
    "!du -sh {INFERENCE_DIR}\n",
    "!du -sh {INFERENCE_DIR}/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zip and download trained model\n",
    "!zip -r hme_model_v2.zip {OUTPUT_DIR} {INFERENCE_DIR}\n",
    "\n",
    "from google.colab import files\n",
    "files.download('hme_model_v2.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Resume Training (if interrupted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resume from last checkpoint\n",
    "!python tools/train.py -c {CONFIG_PATH} \\\n",
    "    -o Global.checkpoints={OUTPUT_DIR}/latest"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
